{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~atplotlib (D:\\anaconda\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (D:\\anaconda\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~atplotlib (D:\\anaconda\\Lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in D:\\anaconda\\Lib\\site-packages (4.9.9)\n",
            "Requirement already satisfied: absl-py in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (2.1.0)\n",
            "Requirement already satisfied: dm-tree in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (0.1.9)\n",
            "Requirement already satisfied: etils>=1.9.1 in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (4.2.2)\n",
            "Requirement already satisfied: numpy in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (1.26.4)\n",
            "Requirement already satisfied: promise in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (6.33.2)\n",
            "Requirement already satisfied: psutil in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
            "Requirement already satisfied: pyarrow in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (21.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (2.32.3)\n",
            "Requirement already satisfied: simple_parsing in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: tensorflow-metadata in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (1.17.3)\n",
            "Requirement already satisfied: termcolor in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (3.2.0)\n",
            "Requirement already satisfied: toml in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (4.66.5)\n",
            "Requirement already satisfied: wrapt in D:\\anaconda\\Lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
            "Requirement already satisfied: einops in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.2)\n",
            "Requirement already satisfied: fsspec in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.14.0)\n",
            "Requirement already satisfied: zipp in D:\\anaconda\\Lib\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in D:\\anaconda\\Lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in D:\\anaconda\\Lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in D:\\anaconda\\Lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in D:\\anaconda\\Lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2025.11.12)\n",
            "Requirement already satisfied: attrs>=18.2.0 in D:\\anaconda\\Lib\\site-packages (from dm-tree->tensorflow_datasets) (23.1.0)\n",
            "Requirement already satisfied: six in D:\\anaconda\\Lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
            "Requirement already satisfied: docstring-parser~=0.15 in D:\\anaconda\\Lib\\site-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in D:\\anaconda\\Lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.70.0)\n",
            "Requirement already satisfied: colorama in D:\\anaconda\\Lib\\site-packages (from tqdm->tensorflow_datasets) (0.4.6)\n",
            "2.20.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Bidirectional, LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "# 1. Caricamento dei dati\n",
        "train_df = pd.read_csv(train_file_path, sep='\\t', names=['label', 'text'])\n",
        "test_df = pd.read_csv(test_file_path, sep='\\t', names=['label', 'text'])\n",
        "\n",
        "# 2. Conversione etichette in numeriche (ham=0, spam=1)\n",
        "train_df['label'] = train_df['label'].map({'ham': 0, 'spam': 1})\n",
        "test_df['label'] = test_df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Parametri per la riproducibilità e scalabilità\n",
        "MAX_FEATURES = 10000  # Dimensione del vocabolario\n",
        "SEQUENCE_LENGTH = 250 # Lunghezza massima dell'SMS\n",
        "BATCH_SIZE = 32\n",
        "# Creazione del layer di vettorizzazione\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=MAX_FEATURES,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH,\n",
        ")\n",
        "\n",
        "# Adattamento del vocabolario ai dati di training\n",
        "vectorizer.adapt(train_df['text'].values)\n",
        "\n",
        "# Creazione dei dataset TensorFlow per performance ottimali\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_df['text'].values, train_df['label'].values)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df['text'].values, test_df['label'].values)\n",
        ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.9301 - loss: 0.2394 - precision: 0.8895 - recall: 0.5464 - val_accuracy: 0.9820 - val_loss: 0.0533 - val_precision: 0.9602 - val_recall: 0.9037 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.9931 - loss: 0.0306 - precision: 0.9819 - recall: 0.9661 - val_accuracy: 0.9878 - val_loss: 0.0376 - val_precision: 0.9775 - val_recall: 0.9305 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - accuracy: 0.9983 - loss: 0.0083 - precision: 0.9982 - recall: 0.9893 - val_accuracy: 0.9885 - val_loss: 0.0388 - val_precision: 0.9777 - val_recall: 0.9358 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - accuracy: 0.9995 - loss: 0.0024 - precision: 1.0000 - recall: 0.9964 - val_accuracy: 0.9885 - val_loss: 0.0363 - val_precision: 0.9572 - val_recall: 0.9572 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9998 - loss: 0.0014 - precision: 1.0000 - recall: 0.9982 - val_accuracy: 0.9842 - val_loss: 0.0444 - val_precision: 0.9275 - val_recall: 0.9572 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9999 - loss: 0.0013 - precision: 1.0000 - recall: 0.9995\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.9998 - loss: 0.0012 - precision: 1.0000 - recall: 0.9982 - val_accuracy: 0.9806 - val_loss: 0.0574 - val_precision: 0.8960 - val_recall: 0.9679 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0011 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9871 - val_loss: 0.0389 - val_precision: 0.9669 - val_recall: 0.9358 - learning_rate: 2.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 5.9458e-04 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.8172e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9871 - val_loss: 0.0401 - val_precision: 0.9669 - val_recall: 0.9358 - learning_rate: 2.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.8828e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9871 - val_loss: 0.0404 - val_precision: 0.9669 - val_recall: 0.9358 - learning_rate: 4.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m130/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 4.7381e-04 - precision: 1.0000 - recall: 1.0000\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.6154e-04 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9871 - val_loss: 0.0406 - val_precision: 0.9669 - val_recall: 0.9358 - learning_rate: 4.0000e-05\n",
            "Epoch 10: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(1,), dtype=tf.string),\n",
        "    vectorizer,\n",
        "    Embedding(MAX_FEATURES, 64, mask_zero=True),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5), # Previene l'overfitting, fondamentale per il CV\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    patience=6, \n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', \n",
        "    factor=0.2, \n",
        "    patience=2, \n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training rapido ma efficace\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=test_ds,\n",
        "    callbacks=[early_stop, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.023942008439917e-05, 'ham']\n",
            "[3.023942008439917e-05, 'ham']\n"
          ]
        }
      ],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "    # 1. Preparazione dell'input come tensore di stringhe\n",
        "    # Avvolgiamo il testo in una lista e poi in un array per creare il batch\n",
        "    input_tensor = tf.constant([pred_text])\n",
        "    \n",
        "    # 2. Inferenza diretta\n",
        "    # Chiamare il modello come una funzione è più veloce e flessibile di .predict()\n",
        "    # per una singola stringa.\n",
        "    prediction = model(input_tensor, training=False)\n",
        "    \n",
        "    # 3. Estrazione del valore\n",
        "    proba = float(prediction[0][0])\n",
        "    \n",
        "    # 4. Logica di classificazione\n",
        "    label = \"spam\" if proba >= 0.5 else \"ham\"\n",
        "    \n",
        "    return [proba, label]\n",
        "\n",
        "# Test manuale\n",
        "pred_text = \"how are you doing today?\"\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)\n",
        "\n",
        "pred_text = \"how are you doing today?\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You passed the challenge. Great job!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_sms_text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
